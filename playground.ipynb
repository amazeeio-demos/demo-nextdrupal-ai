{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain faiss-cpu dotenv openai beautifulsoup4 lark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Datastore - Basic example of text splitting & Document creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "raw_documents = TextLoader('./state_of_the_union.txt').load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store - Data Prep (Tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv('./.env')\n",
    "\n",
    "# Get OPENAI_API_KEY and BEARER_TOKEN from .env file\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# initialize LLM (we use ChatOpenAI because we'll later define a `chat` agent)\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    temperature=0,\n",
    "    model_name='gpt-3.5-turbo'\n",
    ")\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "\"\"\"You are a content tagging bot for a food and drink blog, and your role is to identify and tag recipes. Specify whether the item is a food or drink and then focus on the type (e.g., vegan, gluten-free), unique ingredients (e.g., dried fruit, super seeds), cooking or preparation techniques (e.g., grilling, soaking, mixing), dietary restrictions, cultural origins, meal or occasion types (e.g., breakfast, lunch, dinner, cocktail party), and special flavors or features (e.g., sweet, savory, spicy) that stand out in the given recipe.\n",
    "\n",
    "Recipe: {recipe}\n",
    "\n",
    "Feel free to add any tags that may provide insightful information about the dish or drink. There's no maximum number of tags, so be thorough and descriptive in your tagging, as it helps readers find recipes that match their preferences.\n",
    "YOU MUST return the tags in the following format:\n",
    "\n",
    "Category: Drink, Type: Vegan, Unique Ingredients: Mint leaves, Preparation Techniques: Mixing, Dietary Restrictions: Gluten-free, Cultural Origins: Cuban, Occasion: Cocktail party, Special Features: Refreshing]\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "async def generateTagsFromContent(content):\n",
    "    return chain.invoke({\"recipe\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the JSON API export, clean the HTML, and create a Document object for each page\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "# Open the file for reading\n",
    "with open('./ingest/composetheweb.json', 'r') as file:\n",
    "    jsonapi_export = json.load(file)\n",
    "\n",
    "def clean_html(html_content):\n",
    "    return BeautifulSoup(html_content, 'html.parser').get_text()\n",
    "\n",
    "\n",
    "def create_document(doc):\n",
    "    title = doc['attributes']['title']\n",
    "    source = doc['attributes']['path']['alias']\n",
    "    metadata = {'title': title, 'source': source}\n",
    "    \n",
    "    if doc['type'] == 'node--recipe':\n",
    "        difficulty = str(doc['attributes']['field_difficulty'])\n",
    "        ingredients = str(doc['attributes']['field_ingredients'])\n",
    "        recipe = clean_html(str(doc['attributes']['field_recipe_instruction']['value']))\n",
    "        summary = clean_html(str(doc['attributes']['field_summary']['value']))\n",
    "        \n",
    "        page_content = f\"Title: {title},\\nDifficulty: {difficulty},\\n\\nIngredients: {ingredients},\\n\\nRecipe: {recipe}, \\n\\nSummary: {summary}\"\n",
    "    else:\n",
    "        body = clean_html(str(doc['attributes']['body']['value']))\n",
    "        page_content = f\"Title: {title} - {body}\"\n",
    "    \n",
    "    return Document(metadata=metadata, page_content=page_content)\n",
    "\n",
    "\n",
    "docs = [create_document(doc) for doc in jsonapi_export['data']]\n",
    "\n",
    "\n",
    "generateTagPromises = [generateTagsFromContent(doc.page_content) for doc in docs]\n",
    "tags = await asyncio.gather(*generateTagPromises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the lenth of the docs and the tags\n",
    "print(len(docs))\n",
    "print(len(tags))\n",
    "\n",
    "\n",
    "print(docs[8].page_content)\n",
    "print('------------------')\n",
    "print(tags[8]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updatedDocs = []\n",
    "for index, doc in enumerate(docs):\n",
    "    updatedDocs.append(\n",
    "        Document(\n",
    "            metadata={**doc.metadata, 'tags': tags[index]['text']}, \n",
    "            page_content=doc.page_content\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(updatedDocs[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating vector store...\")\n",
    "vectorstore: FAISS = FAISS.from_documents(updatedDocs, OpenAIEmbeddings())\n",
    "vectorstore.save_local(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query)\n",
    "\n",
    "found_docs = vectorstore.similarity_search_with_relevance_scores(query, n_docs=5, score_threshold=0.7 )\n",
    "found_docs\n",
    "\n",
    "print(found_docs[0][0].metadata['title'])\n",
    "print(found_docs[0][0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Retrievers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Search with relevance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever_with_source(query, n_docs):\n",
    "    hits = vectorstore.similarity_search_with_relevance_scores(query, n_docs, score_threshhold=0.7)\n",
    "    for hit in hits:\n",
    "        hit.metadata[\"source\"] = hit.metadata[\"source\"] \n",
    "    return hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(llm, retriever=vectorstore.as_retriever())\n",
    "\n",
    "result = qa_chain({\"question\": query})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA Chain w/ context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RetrievalQA chain w/ context\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Answer with single sentence description of the dish. Never provide commentary on the context. DO NOT INCLUDE THE RECIPES IN YOUR ANSWER. Finish the answer with a question if the user would like to see the full recipe.\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "result = qa_chain({\"query\": query })\n",
    "print(result['result'])\n",
    "print(result[\"source_documents\"][0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import create_qa_with_sources_chain\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
    "Answer with single sentence description of the dish. Never provide commentary on the context. DO NOT INCLUDE THE RECIPES IN YOUR ANSWER. Finish the answer with a question if the user would like to see the full recipe.\n",
    "{context}\n",
    "\n",
    "History:\n",
    "{chat_history}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Create the multipurpose chain\n",
    "qachat = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    memory=memory,\n",
    "    retriever=retriever, \n",
    "    return_source_documents=True,\n",
    "    condense_question_prompt=QA_CHAIN_PROMPT\n",
    ")\n",
    "\n",
    "question = \"Do we have any recipes for Ukrainian food?\"\n",
    "qachat(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question in your own words as truthfully as possible from the context given to you.\n",
    "If you do not know the answer to the question, simply respond with \"I don't know. Can you ask another question\".\n",
    "If questions are asked where there is no relevant context available, simply respond with \"I don't know. Please ask a question relevant to the documents\"\n",
    "Context: {context}\n",
    "\n",
    "\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"chat_history\", \"question\"], template=template\n",
    ")\n",
    "\n",
    "def get_chat_history(inputs) -> str:\n",
    "    res = []\n",
    "    for human, ai in inputs:\n",
    "        res.append(f\"Human:{human}\\nAI:{ai}\")\n",
    "    return \"\\n\".join(res)\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=vectorstore.as_retriever(), memory=memory,\n",
    "    get_chat_history=get_chat_history, return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={'prompt': prompt})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), return_source_documents=True)\n",
    "chat_history = []\n",
    "query = \"What is the full recipe for the borst?\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "print(result['source_documents'][0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "query = \"Can you provide me with the full recipe?\"\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm, vectorstore.as_retriever(), chat_history=chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoupled",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
